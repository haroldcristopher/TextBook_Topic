{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BQbY46it1Sbz"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "    ROOT = Path('/content/drive/MyDrive/textbook-topic-analysis')\n",
        "except ImportError:\n",
        "    ROOT = Path(\".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: gensim in /Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (4.3.2)\n",
            "Requirement already satisfied: nltk in /Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (3.8.1)\n",
            "Requirement already satisfied: top2vec[sentence_encoders] in /Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (1.0.33)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from beautifulsoup4->-r requirements.txt (line 1)) (2.4.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from gensim->-r requirements.txt (line 2)) (1.24.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from gensim->-r requirements.txt (line 2)) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from gensim->-r requirements.txt (line 2)) (6.4.0)\n",
            "Requirement already satisfied: click in /Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from nltk->-r requirements.txt (line 3)) (8.1.3)\n",
            "Requirement already satisfied: joblib in /Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from nltk->-r requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from nltk->-r requirements.txt (line 3)) (2023.5.5)\n",
            "Requirement already satisfied: tqdm in /Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from nltk->-r requirements.txt (line 3)) (4.65.0)\n",
            "Requirement already satisfied: pandas in /Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from top2vec[sentence_encoders]->-r requirements.txt (line 4)) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from top2vec[sentence_encoders]->-r requirements.txt (line 4)) (1.3.2)\n",
            "Requirement already satisfied: umap-learn>=0.5.1 in /Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from top2vec[sentence_encoders]->-r requirements.txt (line 4)) (0.5.4)\n",
            "Requirement already satisfied: hdbscan>=0.8.27 in /Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from top2vec[sentence_encoders]->-r requirements.txt (line 4)) (0.8.33)\n",
            "Requirement already satisfied: wordcloud in /Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from top2vec[sentence_encoders]->-r requirements.txt (line 4)) (1.9.2)\n",
            "Requirement already satisfied: tensorflow in /Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from top2vec[sentence_encoders]->-r requirements.txt (line 4)) (2.14.0)\n",
            "Collecting tensorflow-hub (from top2vec[sentence_encoders]->-r requirements.txt (line 4))\n",
            "  Using cached tensorflow_hub-0.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "INFO: pip is looking at multiple versions of top2vec[sentence-encoders] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting top2vec[sentence_encoders] (from -r requirements.txt (line 4))\n",
            "  Using cached top2vec-1.0.34-py3-none-any.whl.metadata (18 kB)\n",
            "  Using cached top2vec-1.0.32-py3-none-any.whl.metadata (18 kB)\n",
            "  Using cached top2vec-1.0.31-py3-none-any.whl.metadata (18 kB)\n",
            "  Using cached top2vec-1.0.30-py3-none-any.whl.metadata (18 kB)\n",
            "  Using cached top2vec-1.0.29-py3-none-any.whl (26 kB)\n",
            "  Using cached top2vec-1.0.28-py3-none-any.whl (25 kB)\n",
            "  Using cached top2vec-1.0.27-py3-none-any.whl (25 kB)\n",
            "INFO: pip is still looking at multiple versions of top2vec[sentence-encoders] to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached top2vec-1.0.26-py3-none-any.whl (23 kB)\n",
            "Collecting gensim (from -r requirements.txt (line 2))\n",
            "  Using cached gensim-3.8.3.tar.gz (23.4 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting top2vec[sentence_encoders] (from -r requirements.txt (line 4))\n",
            "  Using cached top2vec-1.0.25-py3-none-any.whl (23 kB)\n",
            "  Using cached top2vec-1.0.24-py3-none-any.whl (22 kB)\n",
            "  Using cached top2vec-1.0.23-py3-none-any.whl (22 kB)\n",
            "  Using cached top2vec-1.0.22-py3-none-any.whl (22 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached top2vec-1.0.21-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pynndescent>=0.4 in /Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from top2vec[sentence_encoders]->-r requirements.txt (line 4)) (0.5.10)\n",
            "Collecting joblib (from nltk->-r requirements.txt (line 3))\n",
            "  Using cached joblib-0.17.0-py3-none-any.whl (301 kB)\n",
            "Collecting numpy>=1.18.5 (from gensim->-r requirements.txt (line 2))\n",
            "  Using cached numpy-1.19.2.zip (7.3 MB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25lerror\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[57 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m Running from numpy source directory.\n",
            "  \u001b[31m   \u001b[0m setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates\n",
            "  \u001b[31m   \u001b[0m   run_build = parse_setuppy_commands()\n",
            "  \u001b[31m   \u001b[0m Cythonizing sources\n",
            "  \u001b[31m   \u001b[0m Processing numpy/random/_bounded_integers.pxd.in\n",
            "  \u001b[31m   \u001b[0m Processing numpy/random/_philox.pyx\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m Error compiling Cython file:\n",
            "  \u001b[31m   \u001b[0m ------------------------------------------------------------\n",
            "  \u001b[31m   \u001b[0m ...\n",
            "  \u001b[31m   \u001b[0m             self.rng_state.ctr.v[i] = counter[i]\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         self._reset_state_variables()\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m         self._bitgen.state = <void *>&self.rng_state\n",
            "  \u001b[31m   \u001b[0m         self._bitgen.next_uint64 = &philox_uint64\n",
            "  \u001b[31m   \u001b[0m                                    ^\n",
            "  \u001b[31m   \u001b[0m ------------------------------------------------------------\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m _philox.pyx:195:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'uint64_t (void *) except? -1 nogil'.\n",
            "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/mw/tbhjsl612qg3bngl8hxs4p100000gn/T/pip-install-wchmkgan/numpy_36a53d5019f848d1b30684e840c0feda/tools/cythonize.py\", line 235, in <module>\n",
            "  \u001b[31m   \u001b[0m     main()\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/mw/tbhjsl612qg3bngl8hxs4p100000gn/T/pip-install-wchmkgan/numpy_36a53d5019f848d1b30684e840c0feda/tools/cythonize.py\", line 231, in main\n",
            "  \u001b[31m   \u001b[0m     find_process_files(root_dir)\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/mw/tbhjsl612qg3bngl8hxs4p100000gn/T/pip-install-wchmkgan/numpy_36a53d5019f848d1b30684e840c0feda/tools/cythonize.py\", line 222, in find_process_files\n",
            "  \u001b[31m   \u001b[0m     process(root_dir, fromfile, tofile, function, hash_db)\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/mw/tbhjsl612qg3bngl8hxs4p100000gn/T/pip-install-wchmkgan/numpy_36a53d5019f848d1b30684e840c0feda/tools/cythonize.py\", line 188, in process\n",
            "  \u001b[31m   \u001b[0m     processor_function(fromfile, tofile)\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/mw/tbhjsl612qg3bngl8hxs4p100000gn/T/pip-install-wchmkgan/numpy_36a53d5019f848d1b30684e840c0feda/tools/cythonize.py\", line 77, in process_pyx\n",
            "  \u001b[31m   \u001b[0m     subprocess.check_call(\n",
            "  \u001b[31m   \u001b[0m   File \"/Users/coby/.pyenv/versions/3.11.3/lib/python3.11/subprocess.py\", line 413, in check_call\n",
            "  \u001b[31m   \u001b[0m     raise CalledProcessError(retcode, cmd)\n",
            "  \u001b[31m   \u001b[0m subprocess.CalledProcessError: Command '['/Users/coby/.pyenv/versions/3.11.3/bin/python3.11', '-m', 'cython', '-3', '--fast-fail', '-o', '_philox.c', '_philox.pyx']' returned non-zero exit status 1.\n",
            "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
            "  \u001b[31m   \u001b[0m   File \"/Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
            "  \u001b[31m   \u001b[0m     main()\n",
            "  \u001b[31m   \u001b[0m   File \"/Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
            "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
            "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/Users/coby/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 149, in prepare_metadata_for_build_wheel\n",
            "  \u001b[31m   \u001b[0m     return hook(metadata_directory, config_settings)\n",
            "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/mw/tbhjsl612qg3bngl8hxs4p100000gn/T/pip-build-env-_puo9asq/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 157, in prepare_metadata_for_build_wheel\n",
            "  \u001b[31m   \u001b[0m     self.run_setup()\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/mw/tbhjsl612qg3bngl8hxs4p100000gn/T/pip-build-env-_puo9asq/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 249, in run_setup\n",
            "  \u001b[31m   \u001b[0m     self).run_setup(setup_script=setup_script)\n",
            "  \u001b[31m   \u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/mw/tbhjsl612qg3bngl8hxs4p100000gn/T/pip-build-env-_puo9asq/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 142, in run_setup\n",
            "  \u001b[31m   \u001b[0m     exec(compile(code, __file__, 'exec'), locals())\n",
            "  \u001b[31m   \u001b[0m   File \"setup.py\", line 499, in <module>\n",
            "  \u001b[31m   \u001b[0m     setup_package()\n",
            "  \u001b[31m   \u001b[0m   File \"setup.py\", line 479, in setup_package\n",
            "  \u001b[31m   \u001b[0m     generate_cython()\n",
            "  \u001b[31m   \u001b[0m   File \"setup.py\", line 274, in generate_cython\n",
            "  \u001b[31m   \u001b[0m     raise RuntimeError(\"Running cythonize failed!\")\n",
            "  \u001b[31m   \u001b[0m RuntimeError: Running cythonize failed!\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "! pip install -r {str(ROOT / \"requirements.txt\")}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU7et3qa2RH6",
        "outputId": "c255b381-4fca-40ea-dbd1-702c05c16643"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "from bs4 import BeautifulSoup, NavigableString\n",
        "from bs4.element import Tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I0zpPkh17F9"
      },
      "source": [
        "# Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ygFlQ8eM1ynA"
      },
      "outputs": [],
      "source": [
        "def join_hyphenated_words(words):\n",
        "    HYPHEN = \"-\"\n",
        "    i = 0\n",
        "    while i < len(words) - 1:\n",
        "        if words[i].endswith(HYPHEN):\n",
        "            words[i] = words[i].rstrip(HYPHEN) + words[i + 1]\n",
        "            del words[i + 1]\n",
        "        else:\n",
        "            i += 1\n",
        "    return words\n",
        "\n",
        "\n",
        "def convert_xml_content_to_string(raw_content: Tag):\n",
        "    content = []\n",
        "    for child in raw_content.find_all(\"ab\", attrs={\"type\": \"Body\"}):\n",
        "        for grandchild in child.children:\n",
        "            if not grandchild.text.strip():\n",
        "                continue\n",
        "            if grandchild.name == \"w\":\n",
        "                content += [text.strip() for text in grandchild.stripped_strings]\n",
        "    return \" \".join(join_hyphenated_words(content))\n",
        "\n",
        "\n",
        "def parse_file(path: Path) -> dict:\n",
        "    \"\"\"\n",
        "    Parses a TEI-encoded XML file into a dictionary of TOC entries -> section contents\n",
        "    \"\"\"\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        soup = BeautifulSoup(f, features=\"xml\")\n",
        "\n",
        "    toc = soup.find(\"front\").find(\"div\", attrs={\"type\": \"contents\"}).find(\"list\")\n",
        "    body = soup.find(\"body\")\n",
        "    index = soup.find(\"div\", attrs={\"type\": \"index\"})\n",
        "\n",
        "    toc_entries = {}\n",
        "    for entry in toc.find_all(\"item\"):\n",
        "        if not entry.find(\"ref\").has_attr(\"target\"):\n",
        "            continue\n",
        "        entry_text = \"\".join(\n",
        "            child for child in entry.contents if isinstance(child, NavigableString)\n",
        "        ).strip()\n",
        "        section_number_match = re.search(r\"\\b\\d+(\\.\\d+)*\\b\", entry_text)\n",
        "        if section_number_match is not None:\n",
        "            section_number = section_number_match.group()\n",
        "            header = entry_text.replace(section_number, \"\").strip()\n",
        "        else:\n",
        "            section_number = None\n",
        "\n",
        "        entry_id = entry.find(\"ref\").attrs[\"target\"]\n",
        "\n",
        "        content_xml = body.find(\"div\", attrs={\"xml:id\": entry_id})\n",
        "        nested_list = entry.find_next_sibling(\"list\")\n",
        "        if nested_list is not None:\n",
        "            subsection_refs = [\n",
        "                ref.attrs[\"target\"]\n",
        "                for ref in nested_list.find_all(\"ref\")\n",
        "                if ref.has_attr(\"target\")\n",
        "            ]\n",
        "        else:\n",
        "            subsection_refs = []\n",
        "\n",
        "        # Exclude content that is contained in a subsection\n",
        "        if content_xml is not None:\n",
        "            for sub_ref in subsection_refs:\n",
        "                sub_contents = content_xml.find_all(\"div\", {\"xml:id\": sub_ref})\n",
        "                if sub_contents is None:\n",
        "                    continue\n",
        "                for sub_content in sub_contents:\n",
        "                    sub_content.decompose()\n",
        "\n",
        "        if content_xml is None:\n",
        "            continue\n",
        "\n",
        "        content_string = convert_xml_content_to_string(content_xml)\n",
        "        word_count = len(content_xml.find_all(\"w\"))\n",
        "\n",
        "        index_refs = index.find_all(\"ref\", attrs={\"xml:id\": entry_id})\n",
        "        index_refs = [] if index_refs is None else index_refs\n",
        "        annotations = [ref.parent for ref in index_refs]\n",
        "\n",
        "        toc_entries[entry_id] = {\n",
        "            \"header\": header,\n",
        "            \"section_number\": section_number,\n",
        "            \"content_xml\": content_xml,\n",
        "            \"content_string\": content_string,\n",
        "            \"word_count\": word_count,\n",
        "            \"subsections\": subsection_refs,\n",
        "            \"annotations\": annotations,\n",
        "            \"similar\": [],\n",
        "        }\n",
        "\n",
        "    return toc_entries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SoupJSONEncoder(json.JSONEncoder):\n",
        "    def default(self, o):\n",
        "        if isinstance(o, Tag):\n",
        "            return str(o)\n",
        "        return super().default(o)\n",
        "\n",
        "\n",
        "def process_file(file):\n",
        "    parsed_file = parse_file(file)\n",
        "    new_file_name = PARSED_TEXTBOOKS_DIRECTORY / (file.stem + \".json\")\n",
        "    with open(new_file_name, \"w\") as f:\n",
        "        json.dump(parsed_file, f, cls=SoupJSONEncoder)\n",
        "\n",
        "\n",
        "PARSED_TEXTBOOKS_DIRECTORY = ROOT / \"parsed_textbooks\"\n",
        "if not PARSED_TEXTBOOKS_DIRECTORY.exists():\n",
        "    PARSED_TEXTBOOKS_DIRECTORY.mkdir()\n",
        "\n",
        "#### CAN COMMENT THIS OUT IF TEXTBOOKS HAVE BEEN PARSED ####\n",
        "# with ThreadPoolExecutor() as executor:\n",
        "#     executor.map(process_file, Path(ROOT / \"textbooks\").glob(\"*.xml\"))\n",
        "############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jjxTJf519eD"
      },
      "source": [
        "# TOC Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "HEADER_TERMS_TO_EXCLUDE = [\"exercises\", \"questions\", \"solutions\"]\n",
        "textbooks = {}\n",
        "for file in Path(PARSED_TEXTBOOKS_DIRECTORY).glob(\"*.json\"):\n",
        "    with open(file) as f:\n",
        "        textbook_json = json.load(f)\n",
        "    parsed_textbook = {}\n",
        "    for entry_id, section in textbook_json.items():\n",
        "        if any(term in section[\"header\"] for term in HEADER_TERMS_TO_EXCLUDE):\n",
        "            continue\n",
        "        if \"content_xml\" not in section:\n",
        "            continue \n",
        "        section[\"content_xml\"] = BeautifulSoup(section[\"content_xml\"], features=\"xml\")\n",
        "        section['similar'] = []\n",
        "        parsed_textbook[entry_id] = section\n",
        "    textbooks[file.stem] = parsed_textbook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_text(text_extraction_fn):\n",
        "    return [\n",
        "        (textbook, entry_id, text_extraction_fn(entry))\n",
        "        for textbook, entries in textbooks.items()\n",
        "        for entry_id, entry in entries.items()\n",
        "    ]\n",
        "\n",
        "\n",
        "def link_similar_entries(texts, is_similar_fn):\n",
        "    result = deepcopy(textbooks)\n",
        "    for textbook1, entry1, text1 in texts:\n",
        "        for textbook2, entry2, text2 in texts:\n",
        "            if textbook1 == textbook2 and entry1 == entry2:\n",
        "                continue\n",
        "            if is_similar_fn(text1, text2):\n",
        "                result[textbook1][entry1][\"similar\"].append(\n",
        "                    {\"textbook\": textbook2, \"entry\": entry2}\n",
        "                )\n",
        "                result[textbook2][entry2][\"similar\"].append(\n",
        "                    {\"textbook\": textbook1, \"entry\": entry1}\n",
        "                )\n",
        "    return result\n",
        "\n",
        "\n",
        "def count_linked(result):\n",
        "    count = 0\n",
        "    for textbook in result.values():\n",
        "        for entry in textbook.values():\n",
        "            count += len(entry[\"similar\"])\n",
        "    return count // 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "def compute_cosine_similarity(text1, text2):\n",
        "    if not text1 or not text2:\n",
        "        return 0\n",
        "    # Vectorizing the text\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
        "    # Calculating cosine similarity\n",
        "    return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Headers only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "headers = extract_text(lambda entry: entry[\"header\"])\n",
        "linked_by_headers = link_similar_entries(\n",
        "    headers, lambda t1, t2: compute_cosine_similarity(t1, t2) > 0.9\n",
        ")\n",
        "count_linked(linked_by_headers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2412"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_contents = extract_text(lambda entry: entry[\"content_string\"])\n",
        "linked_by_text_contents = link_similar_entries(\n",
        "    text_contents, lambda t1, t2: compute_cosine_similarity(t1, t2) > 0.48\n",
        ")\n",
        "count_linked(linked_by_text_contents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Weighted average text and headers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "108"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "headers_and_text_contents = extract_text(\n",
        "    lambda entry: {\"h\": entry[\"header\"], \"c\": entry[\"content_string\"]}\n",
        ")\n",
        "linked_by_headers_and_text_contents = link_similar_entries(\n",
        "    headers_and_text_contents,\n",
        "    lambda t1, t2: np.average(\n",
        "        [\n",
        "            compute_cosine_similarity(t1[\"h\"], t2[\"h\"]),\n",
        "            compute_cosine_similarity(t1[\"c\"], t2[\"c\"]),\n",
        "        ],\n",
        "        weights=[7, 3],\n",
        "    )\n",
        "    > 0.5,\n",
        ")\n",
        "count_linked(linked_by_headers_and_text_contents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Semantic Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Doc2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/coby/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3621"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "\n",
        "def preprocess(doc):\n",
        "    return word_tokenize(doc.lower())\n",
        "\n",
        "\n",
        "train_corpus = [\n",
        "    TaggedDocument(\n",
        "        words=preprocess(section_contents[\"content_string\"]), tags=[(textbook, section)]\n",
        "    )\n",
        "    for textbook, textbook_contents in textbooks.items()\n",
        "    for section, section_contents in textbook_contents.items()\n",
        "    if section_contents[\"content_string\"]\n",
        "]\n",
        "\n",
        "model = Doc2Vec(vector_size=50, min_count=1, epochs=40)\n",
        "model.build_vocab(train_corpus)\n",
        "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "\n",
        "def cos(vec1, vec2):\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "\n",
        "def compute_doc2vec_similarity(doc1, doc2):\n",
        "    vector1 = model.infer_vector(doc1)\n",
        "    vector2 = model.infer_vector(doc2)\n",
        "    similarity = cos(vector1, vector2)\n",
        "    return similarity\n",
        "\n",
        "\n",
        "doc2vec = extract_text(lambda entry: preprocess(entry[\"content_string\"]))\n",
        "linked_by_doc2vec = link_similar_entries(\n",
        "    doc2vec, lambda t1, t2: compute_doc2vec_similarity(t1, t2) > 0.6\n",
        ")\n",
        "count_linked(linked_by_doc2vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "textbook =  2011_Book_StatisticsForNon-Statisticians\n",
            "header = data collection\n",
            "content =\n",
            "    This chapter explains some basic concepts within statistics. Also, we look\n",
            "    at the most important ways to collect data in surveys. Statistics can be\n",
            "    defined as a collection of techniques used when planning a data collection,\n",
            "    and when subsequently analyzing and presenting data. Dating back to ancient\n",
            "    times people have needed knowledge about population size, to carry out a\n",
            "    census of the armies or calculate expected taxes. The word statistics is\n",
            "    derived from the word “status” (originally coming from Latin); and it was\n",
            "    exactly the status of society, which was the subject of the first\n",
            "    statistics! Later emerged probability theory (in connection with games!),\n",
            "    demographics and insurance science as areas, in which statistical thinking\n",
            "    was essential. In today’s digital age it is easy to collect as well as\n",
            "    process and disseminate data, and therefore statistics is used for a variety\n",
            "    of surveys throughout society. Most statistical surveys can be divided into\n",
            "    the following phases: 1. Clarification of concepts 2. Planning of data\n",
            "    collection 3. Data collection 4. Analysis and presentation of data 5.\n",
            "    Conclusion Statistical methods (and statisticians!) are particularly useful\n",
            "    in phases 2 and 4 of the survey. There are two kinds of statistics: –\n",
            "    Descriptive statistics – Analytical statistics Descriptive statistics means\n",
            "    describing data using tables, charts and simple statistical calculations\n",
            "    such as averages, percentages, etc. This is what many people understand by\n",
            "    the word “statistics”. It was also the kind of statistics that was produced\n",
            "    in ancient times. Analytical statistics is used to assess differences and\n",
            "    relationships in data. For example, we could examine whether there is a\n",
            "    relation between height and weight of a group of persons; or whether there\n",
            "    is a difference between height of boys and height of girls, as well as\n",
            "    provide an estimate of how large this difference is. Analytical statistics\n",
            "    is a mathematical discipline, based on calculus of probability. It is a\n",
            "    relatively new discipline that has been developed throughout the twentieth\n",
            "    century. This book is about descriptive statistics as well as analytical\n",
            "    statistics. In practice, you need both. Analytical statistics is a very\n",
            "    large topic, and here we can only scratch the surface (see especially Chaps.\n",
            "    5, 7, and 8). If you want to know more about analytical statistics, see some\n",
            "    of the more advanced books in the literature list.\n",
            "{'header': 'data collection', 'section_number': '1', 'content_string': 'This chapter explains some basic concepts within statistics. Also, we look at the most important ways to collect data in surveys. Statistics can be defined as a collection of techniques used when planning a data collection, and when subsequently analyzing and presenting data. Dating back to ancient times people have needed knowledge about population size, to carry out a census of the armies or calculate expected taxes. The word statistics is derived from the word “status” (originally coming from Latin); and it was exactly the status of society, which was the subject of the first statistics! Later emerged probability theory (in connection with games!), demographics and insurance science as areas, in which statistical thinking was essential. In today’s digital age it is easy to collect as well as process and disseminate data, and therefore statistics is used for a variety of surveys throughout society. Most statistical surveys can be divided into the following phases: 1. Clarification of concepts 2. Planning of data collection 3. Data collection 4. Analysis and presentation of data 5. Conclusion Statistical methods (and statisticians!) are particularly useful in phases 2 and 4 of the survey. There are two kinds of statistics: – Descriptive statistics – Analytical statistics Descriptive statistics means describing data using tables, charts and simple statistical calculations such as averages, percentages, etc. This is what many people understand by the word “statistics”. It was also the kind of statistics that was produced in ancient times. Analytical statistics is used to assess differences and relationships in data. For example, we could examine whether there is a relation between height and weight of a group of persons; or whether there is a difference between height of boys and height of girls, as well as provide an estimate of how large this difference is. Analytical statistics is a mathematical discipline, based on calculus of probability. It is a relatively new discipline that has been developed throughout the twentieth century. This book is about descriptive statistics as well as analytical statistics. In practice, you need both. Analytical statistics is a very large topic, and here we can only scratch the surface (see especially Chaps. 5, 7, and 8). If you want to know more about analytical statistics, see some of the more advanced books in the literature list.', 'word_count': 385, 'subsections': ['seg_3', 'seg_5', 'seg_7', 'seg_9', 'seg_11', 'seg_13', 'seg_15', 'seg_17', 'seg_19', 'seg_21', 'seg_23', 'seg_25'], 'annotations': []}\n",
            "*******\n"
          ]
        }
      ],
      "source": [
        "from textwrap import wrap\n",
        "\n",
        "INDENT = \" \" * 4\n",
        "\n",
        "\n",
        "def print_similar(linked, textbook, section=1):\n",
        "    section = list(linked[textbook].values())[0]\n",
        "    print(\"textbook = \", textbook)\n",
        "    print(\"header =\", section[\"header\"])\n",
        "    print(\"content =\")\n",
        "    for string in wrap(\n",
        "        section[\"content_string\"],\n",
        "        width=80,\n",
        "        initial_indent=INDENT,\n",
        "        subsequent_indent=INDENT,\n",
        "    ):\n",
        "        print(string)\n",
        "    print({k: v for k, v in section.items() if k not in {\"similar\", \"content_xml\"}})\n",
        "    similar = section[\"similar\"]\n",
        "    print(\"*******\")\n",
        "    for s in similar:\n",
        "        similar_entry = linked[s[\"textbook\"]][s[\"entry\"]]\n",
        "        print(\"textbook = \", s[\"textbook\"])\n",
        "        print(\"header =\", similar_entry[\"header\"])\n",
        "        print(\"content =\")\n",
        "        for string in wrap(\n",
        "            similar_entry[\"content_string\"],\n",
        "            width=80,\n",
        "            initial_indent=INDENT,\n",
        "            subsequent_indent=INDENT,\n",
        "        ):\n",
        "            print(string)\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "print_similar(\n",
        "    linked_by_headers_and_text_contents, \"2011_Book_StatisticsForNon-Statisticians\", section=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4I0zpPkh17F9"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
