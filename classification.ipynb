{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from computation.expert import expert_integration\n",
    "from computation.pipeline import pipeline_integration\n",
    "from textbooks.data import Textbook\n",
    "from textbooks.utils import extract_concept_name, extract_content\n",
    "\n",
    "BASE_TEXTBOOK = \"2012_Book_ModernMathematicalStatisticsWi\"\n",
    "EXPERT_OTHER_TEXTBOOK = \"Walpole_Probability_and_Statistics\"\n",
    "\n",
    "\n",
    "best_integration_config = {\n",
    "    \"tfidf_text_extraction_fns\": [extract_content, extract_concept_name],\n",
    "    \"tfidf_threshold\": 0.6,\n",
    "    \"tfidf_uncertain_threshold\": 0.3,\n",
    "    \"d2v_text_extraction_fn\": extract_content,\n",
    "    \"d2v_threshold\": 0.3,\n",
    "    \"d2v_vector_size\": 100,\n",
    "    \"d2v_min_count\": 1,\n",
    "    \"evaluate\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_dataset = expert_integration(\n",
    "    base_textbook=Textbook.from_json(f\"textbooks-parsed/{BASE_TEXTBOOK}.json\"),\n",
    "    other_textbooks=(\n",
    "        Textbook.from_json(f\"textbooks-parsed/{EXPERT_OTHER_TEXTBOOK}.json\"),\n",
    "    ),\n",
    ").dataset\n",
    "\n",
    "print(\"Number of topic labels:\", len(set(d[\"topic\"] for d in expert_dataset)))\n",
    "print(\"Number of data points:\", len(expert_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_textbook = pipeline_integration(\n",
    "    base_textbook=Textbook.from_json(f\"textbooks-parsed/{BASE_TEXTBOOK}.json\"),\n",
    "    other_textbooks=(\n",
    "        Textbook.from_json(f\"textbooks-parsed/{EXPERT_OTHER_TEXTBOOK}.json\"),\n",
    "    ),\n",
    "    **best_integration_config,\n",
    ")\n",
    "small_generated_dataset = integrated_textbook.dataset\n",
    "\n",
    "print(\"Number of topic labels:\", len(set(d[\"topic\"] for d in small_generated_dataset)))\n",
    "print(\"Number of data points:\", len(small_generated_dataset))\n",
    "\n",
    "with open(\"small-dataset.json\", \"w\") as f:\n",
    "    json.dump(small_generated_dataset, f)\n",
    "\n",
    "with open(\"small-dataset.json\") as f:\n",
    "    small_generated_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base_textbook_path = Path(f\"textbooks-parsed/{BASE_TEXTBOOK}.json\")\n",
    "base_textbook = Textbook.from_json(base_textbook_path)\n",
    "other_textbooks = [\n",
    "    Textbook.from_json(p)\n",
    "    for p in Path(\"textbooks-parsed\").glob(\"*\")\n",
    "    if p != base_textbook_path\n",
    "]\n",
    "\n",
    "integrated_textbook = pipeline_integration(\n",
    "    base_textbook,\n",
    "    other_textbooks,\n",
    "    **best_integration_config,\n",
    ")\n",
    "large_generated_dataset = integrated_textbook.dataset\n",
    "\n",
    "print(\"Number of topic labels:\", len(set(d[\"topic\"] for d in large_generated_dataset)))\n",
    "print(\"Number of data points:\", len(large_generated_dataset))\n",
    "\n",
    "with open(\"large-dataset.json\", \"w\") as f:\n",
    "    json.dump(large_generated_dataset, f)\n",
    "\n",
    "with open(\"large-dataset.json\") as f:\n",
    "    large_generated_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning & cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fine tuning\n",
    "from keras.layers import LSTM, SimpleRNN\n",
    "\n",
    "from classification.validation import advanced_language_model_cv\n",
    "\n",
    "\n",
    "def grid_search_results_serializer(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if hasattr(obj, \"__name__\"):\n",
    "        return obj.__name__\n",
    "    raise TypeError(f\"Object of type {obj.__class__.__name__} is not JSON serializable\")\n",
    "\n",
    "\n",
    "wide_param_grid = {\n",
    "    \"model__units\": [100, 125, 150, 200],\n",
    "    \"model__dropout_rate\": [0.4, 0.6, 0.8, 0.9],\n",
    "    \"model__model_type\": [LSTM, SimpleRNN],\n",
    "    \"batch_size\": [32, 64, 128],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Expert data with concepts\n",
    "results_summary, cv_results = advanced_language_model_cv(\n",
    "    expert_dataset, wide_param_grid, with_concepts=True\n",
    ")\n",
    "with open(\"results/phase2/expert-with.json\", \"w\") as f:\n",
    "    json.dump(cv_results, f, default=grid_search_results_serializer)\n",
    "results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Expert data without concepts\n",
    "results_summary, cv_results = advanced_language_model_cv(\n",
    "    expert_dataset, wide_param_grid, with_concepts=False\n",
    ")\n",
    "with open(\"results/phase2/expert-without.json\", \"w\") as f:\n",
    "    json.dump(cv_results, f, default=grid_search_results_serializer)\n",
    "results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Small generated data with concepts\n",
    "results_summary, cv_results = advanced_language_model_cv(\n",
    "    small_generated_dataset, wide_param_grid, with_concepts=True\n",
    ")\n",
    "with open(\"results/phase2/small-with.json\", \"w\") as f:\n",
    "    json.dump(cv_results, f, default=grid_search_results_serializer)\n",
    "results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Small generated data without concepts\n",
    "results_summary, cv_results = advanced_language_model_cv(\n",
    "    small_generated_dataset, wide_param_grid, with_concepts=False\n",
    ")\n",
    "with open(\"results/phase2/small-without.json\", \"w\") as f:\n",
    "    json.dump(cv_results, f, default=grid_search_results_serializer)\n",
    "results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narrow_param_grid = {\n",
    "    \"model__units\": [100, 125, 150],\n",
    "    \"model__dropout_rate\": [0.4, 0.6, 0.8, 0.9],\n",
    "    \"model__model_type\": [SimpleRNN],\n",
    "    \"batch_size\": [32, 64],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Large generated data with concepts\n",
    "results_summary, cv_results = advanced_language_model_cv(\n",
    "    large_generated_dataset, narrow_param_grid, with_concepts=True\n",
    ")\n",
    "with open(\"results/phase2/large-with.json\", \"w\") as f:\n",
    "    json.dump(cv_results, f, default=grid_search_results_serializer)\n",
    "results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Large generated data without concepts\n",
    "results_summary, cv_results = advanced_language_model_cv(\n",
    "    large_generated_dataset, narrow_param_grid, with_concepts=False\n",
    ")\n",
    "with open(\"results/phase2/large-without.json\", \"w\") as f:\n",
    "    json.dump(cv_results, f, default=grid_search_results_serializer)\n",
    "results_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from bertopic.dimensionality import BaseDimensionalityReduction\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from utils import performance_metrics\n",
    "\n",
    "\n",
    "def append(data):\n",
    "    with open(\"results/phase2/bertopic.jsonl\", \"a\") as f:\n",
    "        f.write(json.dumps(data, default=grid_search_results_serializer) + \"\\n\")\n",
    "\n",
    "\n",
    "def bertopic_cv(dataset, with_concepts, n_splits=5):\n",
    "    topics = [d[\"topic\"] for d in dataset]\n",
    "    le = LabelEncoder()\n",
    "    encoded_topics = le.fit_transform(topics)\n",
    "\n",
    "    docs = [\n",
    "        f\"{d['concepts']} {d['content']}\" if with_concepts else d[\"content\"]\n",
    "        for d in dataset\n",
    "    ]\n",
    "    y = encoded_topics\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "\n",
    "    for train_index, test_index in kf.split(docs):\n",
    "        X_train, X_test = [docs[i] for i in train_index], [docs[i] for i in test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        empty_dimensionality_model = BaseDimensionalityReduction()\n",
    "        clf = LogisticRegression()\n",
    "        ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "        topic_model = BERTopic(\n",
    "            umap_model=empty_dimensionality_model,\n",
    "            hdbscan_model=clf,\n",
    "            ctfidf_model=ctfidf_model,\n",
    "        )\n",
    "        topic_model.fit(X_train, y=y_train)\n",
    "        y_pred, _ = topic_model.transform(X_test)\n",
    "        results.append(performance_metrics(y_test, y_pred))\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Expert data with concepts\n",
    "cv_results_expert = bertopic_cv(expert_dataset, with_concepts=True)\n",
    "results_summary = cv_results_expert.mean().to_dict()\n",
    "append(\n",
    "    results_summary | {\"model__model_type\": \"BERTopic\", \"dataset\": \"expert\", \"concepts\": True}\n",
    ")\n",
    "results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Expert data without concepts\n",
    "cv_results_expert = bertopic_cv(expert_dataset, with_concepts=False)\n",
    "results_summary = cv_results_expert.mean().to_dict()\n",
    "append(\n",
    "    results_summary | {\"model__model_type\": \"BERTopic\", \"dataset\": \"expert\", \"concepts\": False}\n",
    ")\n",
    "results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Large generated data with concepts\n",
    "cv_results_large = bertopic_cv(large_generated_dataset, with_concepts=True)\n",
    "results_summary = cv_results_large.mean().to_dict()\n",
    "append(\n",
    "    results_summary | {\"model__model_type\": \"BERTopic\", \"dataset\": \"large\", \"concepts\": True}\n",
    ")\n",
    "results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Large generated data without concepts\n",
    "cv_results_large = bertopic_cv(large_generated_dataset, with_concepts=False)\n",
    "results_summary = cv_results_large.mean().to_dict()\n",
    "append(\n",
    "    results_summary | {\"model__model_type\": \"BERTopic\", \"dataset\": \"large\", \"concepts\": False}\n",
    ")\n",
    "results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Small generated data with concepts\n",
    "cv_results_small = bertopic_cv(small_generated_dataset, with_concepts=True)\n",
    "results_summary = cv_results_small.mean().to_dict()\n",
    "append(\n",
    "    results_summary | {\"model__model_type\": \"BERTopic\", \"dataset\": \"small\", \"concepts\": True}\n",
    ")\n",
    "results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Small generated data without concepts\n",
    "cv_results_small_generated = bertopic_cv(small_generated_dataset, with_concepts=False)\n",
    "results_summary = cv_results_small.mean().to_dict()\n",
    "append(\n",
    "    results_summary | {\"model__model_type\": \"BERTopic\", \"dataset\": \"small\", \"concepts\": False}\n",
    ")\n",
    "results_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
