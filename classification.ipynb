{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate embeddings for each section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from computations.doc2vec import doc2vec_integration\n",
    "from computations.expert import expert_integration\n",
    "from textbooks.data import Textbook\n",
    "from textbooks.utils import extract_content\n",
    "\n",
    "base_textbook = Textbook.from_json(\n",
    "    Path(\"textbooks-parsed/2012_Book_ModernMathematicalStatisticsWi.json\")\n",
    ")\n",
    "other_textbooks = [\n",
    "    Textbook.from_json(Path(\"textbooks-parsed/Walpole_Probability_and_Statistics.json\"))\n",
    "]\n",
    "\n",
    "integrated_textbook = doc2vec_integration(\n",
    "    base_textbook,\n",
    "    other_textbooks,\n",
    "    text_extraction_fn=extract_content,\n",
    "    threshold=0.4,\n",
    "    vector_size=100,\n",
    "    min_count=1,\n",
    "    epochs=40,\n",
    "    iterative=False,\n",
    "    evaluate=False,\n",
    ")\n",
    "\n",
    "expert_it = expert_integration(base_textbook, other_textbooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification.bert import run_bert\n",
    "\n",
    "run_bert(integrated_textbook, \"bert-vectors/doc2vec.json\")\n",
    "run_bert(expert_it, \"bert-vectors/expert.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find best model using expert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "from classification.preprocess import preprocess_data\n",
    "\n",
    "\n",
    "def load_vectors(filename):\n",
    "    with open(filename) as f:\n",
    "        vectors = json.load(f)\n",
    "\n",
    "    X = tf.convert_to_tensor([v[\"x\"] for v in vectors])\n",
    "    y = tf.convert_to_tensor([v[\"y\"] for v in vectors])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X, y = load_vectors(\"bert-vectors/expert.json\")\n",
    "num_classes, X_train, X_test, y_train, y_test = preprocess_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, SimpleRNN\n",
    "\n",
    "from classification.neural_nets import grid_search_neural_networks\n",
    "\n",
    "\n",
    "def reshape(array):\n",
    "    return array.reshape(-1, 1, array.shape[-1])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"model__units\": [100, 125, 150, 200],\n",
    "    \"model__dropout_rate\": [0.4, 0.6, 0.8, 0.9],\n",
    "    \"model__model_type\": [LSTM, SimpleRNN],\n",
    "    \"batch_size\": [32, 64, 128],\n",
    "}\n",
    "\n",
    "expert_model = grid_search_neural_networks(\n",
    "    num_classes=num_classes,\n",
    "    X_train=reshape(X_train),\n",
    "    y_train=y_train,\n",
    "    param_grid=param_grid,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def test(model):\n",
    "    y_pred = model.predict(reshape(X_test))\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=\"micro\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"micro\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "test(expert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find best model using computer-generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_vectors(\"bert-vectors/doc2vec.json\")\n",
    "num_classes, X_train, X_test, y_train, y_test = preprocess_data(X, y)\n",
    "\n",
    "computer_model = grid_search_neural_networks(\n",
    "    num_classes=num_classes,\n",
    "    X_train=reshape(X_train),\n",
    "    y_train=y_train,\n",
    "    param_grid=param_grid,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(computer_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
